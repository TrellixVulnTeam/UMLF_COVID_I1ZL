{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.5.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 1.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'keras_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c24483574c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# TODO: Remove once https://github.com/tensorflow/tensorflow/issues/44613 is resolved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'keras_tensor'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "\n",
    "from models.maml.maml import ModelAgnosticMetaLearningModel\n",
    "from networks import SimpleModel, MiniImagenetModel\n",
    "from tf_datasets import OmniglotDatabase, MiniImagenetDatabase\n",
    "\n",
    "\n",
    "class UMTRA(ModelAgnosticMetaLearningModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            database,\n",
    "            network_cls,\n",
    "            n,\n",
    "            meta_batch_size,\n",
    "            num_steps_ml,\n",
    "            lr_inner_ml,\n",
    "            num_steps_validation,\n",
    "            save_after_epochs,\n",
    "            meta_learning_rate,\n",
    "            report_validation_frequency,\n",
    "            log_train_images_after_iteration,  # Set to -1 if you do not want to log train images.\n",
    "            least_number_of_tasks_val_test=-1,  # Make sure the val and test dataset pick at least this many tasks.\n",
    "            clip_gradients=False,\n",
    "            augmentation_function=None\n",
    "    ):\n",
    "        self.augmentation_function = augmentation_function\n",
    "        super(UMTRA, self).__init__(\n",
    "            database=database,\n",
    "            network_cls=network_cls,\n",
    "            n=n,\n",
    "            k=1,\n",
    "            meta_batch_size=meta_batch_size,\n",
    "            num_steps_ml=num_steps_ml,\n",
    "            lr_inner_ml=lr_inner_ml,\n",
    "            num_steps_validation=num_steps_validation,\n",
    "            save_after_epochs=save_after_epochs,\n",
    "            meta_learning_rate=meta_learning_rate,\n",
    "            report_validation_frequency=report_validation_frequency,\n",
    "            log_train_images_after_iteration=log_train_images_after_iteration,\n",
    "            least_number_of_tasks_val_test=least_number_of_tasks_val_test,\n",
    "            clip_gradients=clip_gradients\n",
    "        )\n",
    "\n",
    "    def get_root(self):\n",
    "        return os.path.dirname(__file__)\n",
    "\n",
    "    def get_train_dataset(self):\n",
    "        dataset = self.database.get_umtra_dataset(\n",
    "            self.database.train_folders,\n",
    "            n=self.n,\n",
    "            meta_batch_size=self.meta_batch_size,\n",
    "            augmentation_function=self.augmentation_function\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_config_info(self):\n",
    "        return f'umtra_' \\\n",
    "               f'model-{self.network_cls.name}_' \\\n",
    "               f'mbs-{self.meta_batch_size}_' \\\n",
    "               f'n-{self.n}_' \\\n",
    "               f'k-{self.k}_' \\\n",
    "               f'stp-{self.num_steps_ml}'\n",
    "\n",
    "\n",
    "def run_omniglot():\n",
    "    @tf.function\n",
    "    def augment(images):\n",
    "        result = list()\n",
    "        num_imgs = 1\n",
    "        for i in range(images.shape[0]):\n",
    "            image = tf.reshape(images[i], (num_imgs, 28, 28, 1))\n",
    "            random_map = tf.random.uniform(shape=tf.shape(image), minval=0, maxval=2, dtype=tf.int32)\n",
    "            random_map = tf.cast(random_map, tf.float32)\n",
    "            image = tf.minimum(image, random_map)\n",
    "\n",
    "            base_ = tf.convert_to_tensor(np.tile([1, 0, 0, 0, 1, 0, 0, 0], [num_imgs, 1]), dtype=tf.float32)\n",
    "            mask_ = tf.convert_to_tensor(np.tile([0, 0, 1, 0, 0, 1, 0, 0], [num_imgs, 1]), dtype=tf.float32)\n",
    "            random_shift_ = tf.random.uniform([num_imgs, 8], minval=-6., maxval=6., dtype=tf.float32)\n",
    "            transforms_ = base_ + random_shift_ * mask_\n",
    "            augmented_data = tfa.image.transform(images=image, transforms=transforms_)\n",
    "            result.append(augmented_data)\n",
    "\n",
    "        return tf.stack(result)\n",
    "\n",
    "    omniglot_database = OmniglotDatabase(\n",
    "        random_seed=-1,\n",
    "        num_train_classes=1200,\n",
    "        num_val_classes=100,\n",
    "    )\n",
    "\n",
    "    umtra = UMTRA(\n",
    "        database=omniglot_database,\n",
    "        network_cls=SimpleModel,\n",
    "        n=5,\n",
    "        meta_batch_size=32,\n",
    "        num_steps_ml=5,\n",
    "        lr_inner_ml=0.01,\n",
    "        num_steps_validation=5,\n",
    "        save_after_epochs=5,\n",
    "        meta_learning_rate=0.001,\n",
    "        log_train_images_after_iteration=10,\n",
    "        report_validation_frequency=1,\n",
    "        augmentation_function=augment\n",
    "    )\n",
    "\n",
    "    umtra.train(epochs=10)\n",
    "    umtra.evaluate(iterations=50)\n",
    "\n",
    "\n",
    "def run_mini_imagenet():\n",
    "    mini_imagenet_database = MiniImagenetDatabase(random_seed=-1)\n",
    "\n",
    "    @tf.function\n",
    "    def augment(images):\n",
    "        images = tf.squeeze(images, axis=1)\n",
    "\n",
    "        if tf.random.uniform(shape=(), minval=0, maxval=1) > 0.5:\n",
    "            images = tf.image.rgb_to_grayscale(images)\n",
    "            images = tf.squeeze(images, axis=-1)\n",
    "            images = tf.stack((images, images, images), axis=-1)\n",
    "        else:\n",
    "            images = tf.image.random_brightness(images, max_delta=0.4)\n",
    "            images = tf.image.random_hue(images, max_delta=0.4)\n",
    "\n",
    "        if tf.random.uniform(shape=(), minval=0, maxval=1) > 0.7:\n",
    "            random_map = tf.random.uniform(shape=tf.shape(images)[:-1], minval=0, maxval=2, dtype=tf.int32)\n",
    "            random_map = tf.stack((random_map, random_map, random_map), axis=-1)\n",
    "            random_map = tf.cast(random_map, tf.float32)\n",
    "            images = tf.minimum(images, random_map)\n",
    "\n",
    "        if tf.random.uniform(shape=(), minval=0, maxval=1) > 0.5:\n",
    "            transforms = [\n",
    "                1,\n",
    "                0,\n",
    "                -tf.random.uniform(shape=(), minval=-40, maxval=40, dtype=tf.int32),\n",
    "                0,\n",
    "                1,\n",
    "                -tf.random.uniform(shape=(), minval=-40, maxval=40, dtype=tf.int32),\n",
    "                0,\n",
    "                0\n",
    "            ]\n",
    "            images = tfa.image.transform(images, transforms)\n",
    "\n",
    "        if tf.random.uniform(shape=(), minval=0, maxval=1) > 0.7:\n",
    "            images = tfa.image.rotate(images, tf.random.uniform(shape=(5, ), minval=-30, maxval=30))\n",
    "\n",
    "        if tf.random.uniform(shape=(), minval=0, maxval=1) > 0.7:\n",
    "            images = tf.image.random_crop(images, size=(5, 42, 42, 3))\n",
    "            images = tf.image.resize(images, size=(84, 84))\n",
    "\n",
    "        return tf.reshape(images, (5, 1, 84, 84, 3))\n",
    "\n",
    "    umtra = UMTRA(\n",
    "        database=mini_imagenet_database,\n",
    "        network_cls=MiniImagenetModel,\n",
    "        n=5,\n",
    "        meta_batch_size=16,\n",
    "        num_steps_ml=1,\n",
    "        lr_inner_ml=0.05,\n",
    "        num_steps_validation=1,\n",
    "        save_after_epochs=1,\n",
    "        meta_learning_rate=0.01,\n",
    "        report_validation_frequency=1,\n",
    "        log_train_images_after_iteration=10,\n",
    "        least_number_of_tasks_val_test=100,\n",
    "        clip_gradients=True,\n",
    "        augmentation_function=augment\n",
    "    )\n",
    "\n",
    "    umtra.train(epochs=40)\n",
    "\n",
    "    umtra = UMTRA(\n",
    "        database=mini_imagenet_database,\n",
    "        network_cls=MiniImagenetModel,\n",
    "        n=5,\n",
    "        meta_batch_size=16,\n",
    "        num_steps_ml=1,\n",
    "        lr_inner_ml=0.05,\n",
    "        num_steps_validation=1,\n",
    "        save_after_epochs=1,\n",
    "        meta_learning_rate=0.001,\n",
    "        report_validation_frequency=1,\n",
    "        log_train_images_after_iteration=10,\n",
    "        least_number_of_tasks_val_test=100,\n",
    "        clip_gradients=True,\n",
    "        augmentation_function=augment\n",
    "    )\n",
    "    return umtra.evaluate(iterations=50, epochs_to_load_from=3000)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_omniglot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
